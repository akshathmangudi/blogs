<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <title></title>
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="stylesheet" href="../css/main.css">
    </head>
    <body>
        <a href="../src/eps3.html">episode 3</a>
        <a class="last" href="../index.html">home</a>
        <h1>running large llms on potato pcs</h1>
        <p>
            as you set out to become the best deep learning engineer or whatever 
            you plan on doing so in the field, there will be a desire to run these large
            models on your pc. but, if youre unlucky like me with a terrible graphics card, 
            you might not be able to do so. your hopes of running llama3-405b just went down the
            gutter :P 
            <br><br>
            however, there is a solution. though you might not hvae access to a A100 collecting
            dust in your attic, what you could do is to <strong>quantize</strong> the llms you're
            trying to run and pray that it works.
        </p>
        <h2>but wait, what is quantization?</h2>
        <p>
            quantization actually comes from digital signal processing, where we compress
            a range of values into a single value, kind of like the opposite of fuzzification. 
            <br><br>
            so how do we apply it here?
            <br><br>
            for that, we have to briefly understand how model weights are actually stored. so let's 
            delve into some cool math. 
        </p>
        <h2>cool math time.</h2>
        <p>
            all model weights are stored in single-precision (FP32) or nowadays, in half-precision (FP16)
            datatype. And respectively, these take 4 bytes and 2 bytes. So let's do some math. 
            <br><br>
            for example: let us take some generic-70b model. Now, during computation, the weights
            of all 70 billion parameters are stored on the gpu (probably why they use multiple gpus, but 
            that's not the scope here.), so: 
            <br><br>
            70,000,000,000 (70 billion) x 4 bytes per param: 280,000,000,000 bytes (or) 280 GB of memory/vram 
            of the GPU. 
            <br><br>
            this, is a HUGE amount. good luck trying to find the moeny to fund yourself for that. but this is where
            quantization comes into the picture. 
            <br><br>
            with quantization, we can basically tradeoff a bit of accuracy for precision, so we can convert it from 
            single-precision to int8 format, which only takes 1 byte, compared to the 4 bytes initially. so then: 
            <br><br>
            70,000,000,000 (70 billion) x 1 byte per param: 70,000,000,000 bytes (or) 70 GB of memory. That's way 
            better! obviously, you still need a high-end gpu to run this, it's still better than the unquantized 
            model. 
        </p>
        <h2>tradeoff.</h2>
        <p>
            it's important to understand that through quantization, you lose accuracy. the more aggressive the 
            quantization, the more you lose in accuracy. though there are ways to overcome this, that should be kept
            in mind. we have different kinds of quantization: 
            <ul>
                <li>post-training quantization (ptq)</li>
                <li>quantization-aware training (qat)</li>
            </ul>

            <strong>post-training quantization</strong> as the name suggests, quantizes the llm after training is complete. 
            this is the cheapest way to quantize an llm, but it is also more prone to loss of accuracy. 
            <strong>quantization-aware training</strong> integrates the quantization process within training itself, this has a 
            loss function of its own and post-training, weights will already be quantized with the least loss in accuracy. 
            <br><br>
            qat is always better than ptq as it accounts for minimization of loss and hence accuracy will not be a huge disadvantage.
        </p>
        <h2>other methods.</h2>
        <p>
            there are other quantization methods as well such as qlora, gptq, ggml/uf, awq, etc. let's briefly talk about them: 

            <ul>
                <li>
                    qlora (quantized low-rank adaptation) is a technique where the original weights of the base llm are quantized to 4-bit
                    to run it on a single GPU.
                </li>
                <li>
                    gptq (generative pre-trained transformer quantization) quantizes a model layer at a time, and also keeps track 
                    of the output error and tries to minimize it.
                </li>
                <li>
                    ggml/uf is a quantization technique and a model format to be stored, such that llms can be run on CPUs, as opposed to needing
                    GPUs.
                </li>
                <li>
                    awq keeps a certain percentage of the weights and quantizes the rest to prevent quantization 
                    loss or without performance degradation.
                </li>
            </ul>
        </p>
        <h2>
            notes and resources.
        </h2>
        <p>
            so now that you know a little bit as to how quantization works, you could try quantizing an llm yourself and be happy about 
            the fact that you can run larger llms on your computer without spending thousands on acquiring a gpu. 

            <ul>
                <li>more on quantization: <a href="https://huggingface.co/docs/transformers/main/en/quantization/overview">huggingface</a></li>
                <li>how to quantize llms: <a href="https://www.tensorops.ai/post/what-are-quantized-llms">quantization</a></li>
            </ul>
        </p>
    </body>
    <footer>
        thanks for reading.
    </footer>
</html>